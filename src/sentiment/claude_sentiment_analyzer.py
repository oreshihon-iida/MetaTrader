#!/usr/bin/env python3
"""
Claude Sentiment Analyzer for Forex Trading
Claude Codeãƒ™ãƒ¼ã‚¹ã®ç„¡æ–™æ„Ÿæƒ…åˆ†æã‚·ã‚¹ãƒ†ãƒ 
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
import json
import os
import re

class ClaudeSentimentAnalyzer:
    """
    Claude Codeæ„Ÿæƒ…åˆ†æã‚·ã‚¹ãƒ†ãƒ 
    """
    
    def __init__(self, sentiment_cache_path: str = "sentiment_cache.json"):
        self.sentiment_cache_path = sentiment_cache_path
        self.sentiment_cache = self._load_sentiment_cache()
        
        # æ„Ÿæƒ…åˆ†æã‚¹ã‚³ã‚¢ã®ç¯„å›²
        self.score_range = (-1.0, 1.0)
        
        # é‡è¦ã‚¤ãƒ™ãƒ³ãƒˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        self.important_keywords = {
            'fed_keywords': ['FRB', 'FOMC', 'åˆ©ä¸Šã’', 'åˆ©ä¸‹ã’', 'ãƒ‘ã‚¦ã‚¨ãƒ«', 'é‡‘èæ”¿ç­–'],
            'boj_keywords': ['æ—¥éŠ€', 'é»’ç”°', 'æ¤ç”°', 'é‡‘èç·©å’Œ', 'YCC', 'é‡‘èæ”¿ç­–æ±ºå®šä¼šåˆ'],
            'economic_keywords': ['é›‡ç”¨çµ±è¨ˆ', 'GDP', 'CPI', 'ã‚¤ãƒ³ãƒ•ãƒ¬', 'PCE', 'ISM'],
            'geopolitical_keywords': ['ã‚¦ã‚¯ãƒ©ã‚¤ãƒŠ', 'ãƒ­ã‚·ã‚¢', 'ä¸­æ±', 'å°æ¹¾', 'ä¸­å›½', 'åœ°æ”¿å­¦'],
            'market_keywords': ['æ ªä¾¡', 'VIX', 'ãƒªã‚¹ã‚¯ã‚ªãƒ³', 'ãƒªã‚¹ã‚¯ã‚ªãƒ•', 'å††å®‰', 'å††é«˜']
        }
    
    def _load_sentiment_cache(self) -> Dict:
        """æ„Ÿæƒ…åˆ†æã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®èª­ã¿è¾¼ã¿"""
        if os.path.exists(self.sentiment_cache_path):
            try:
                with open(self.sentiment_cache_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return {}
    
    def _save_sentiment_cache(self):
        """æ„Ÿæƒ…åˆ†æã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ä¿å­˜"""
        try:
            with open(self.sentiment_cache_path, 'w', encoding='utf-8') as f:
                json.dump(self.sentiment_cache, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def analyze_news_importance(self, news_text: str) -> float:
        """
        ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®é‡è¦åº¦ã‚’è‡ªå‹•åˆ¤å®š
        """
        importance_score = 0.0
        news_lower = news_text.lower()
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥é‡è¦åº¦
        category_weights = {
            'fed_keywords': 0.9,      # FRBé–¢é€£ã¯æœ€é‡è¦
            'boj_keywords': 0.8,      # æ—¥éŠ€é–¢é€£ã‚‚é‡è¦
            'economic_keywords': 0.7,  # çµŒæ¸ˆæŒ‡æ¨™
            'geopolitical_keywords': 0.6,  # åœ°æ”¿å­¦
            'market_keywords': 0.5     # å¸‚å ´é–¢é€£
        }
        
        for category, keywords in self.important_keywords.items():
            for keyword in keywords:
                if keyword.lower() in news_lower:
                    importance_score = max(importance_score, category_weights[category])
        
        return min(importance_score, 1.0)
    
    def generate_claude_analysis_prompt(self, news_text: str, forex_pair: str = "USDJPY") -> str:
        """
        Claude Codeç”¨ã®æ„Ÿæƒ…åˆ†æãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
        """
        prompt = f"""
ä»¥ä¸‹ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒ{forex_pair}é€šè²¨ãƒšã‚¢ã«ä¸ãˆã‚‹å½±éŸ¿ã‚’å°‚é–€çš„ã«åˆ†æã—ã¦ãã ã•ã„ï¼š

ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹ã€‘
{news_text}

ã€åˆ†æé …ç›®ã€‘
1. ç·åˆæ„Ÿæƒ…ã‚¹ã‚³ã‚¢: -1.0ã€œ+1.0 ï¼ˆå¼·ã„ãƒã‚¬ãƒ†ã‚£ãƒ–ã€œå¼·ã„ãƒã‚¸ãƒ†ã‚£ãƒ–ï¼‰
2. USDå½±éŸ¿åº¦: -1.0ã€œ+1.0 ï¼ˆUSDå¼±ã„ã€œUSDå¼·ã„ï¼‰
3. JPYå½±éŸ¿åº¦: -1.0ã€œ+1.0 ï¼ˆJPYå¼±ã„ã€œJPYå¼·ã„ï¼‰
4. æ™‚é–“è»¸: short/medium/long ï¼ˆå½±éŸ¿ã®æŒç¶šæœŸé–“ï¼‰
5. ä¿¡é ¼åº¦: 0.0ã€œ1.0 ï¼ˆåˆ†æã®ç¢ºä¿¡åº¦ï¼‰
6. ä¸»è¦è¦å› : ã“ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®æ ¸å¿ƒçš„è¦ç´ 

ã€å›ç­”å½¢å¼ã€‘
```json
{{
    "sentiment_score": 0.0,
    "usd_impact": 0.0, 
    "jpy_impact": 0.0,
    "timeframe": "short",
    "confidence": 0.0,
    "key_factors": ["è¦å› 1", "è¦å› 2"]
}}
```

FXãƒˆãƒ¬ãƒ¼ãƒ€ãƒ¼ã®è¦–ç‚¹ã§ã€å®Ÿéš›ã®å–å¼•åˆ¤æ–­ã«ä½¿ãˆã‚‹åˆ†æã‚’ãŠé¡˜ã„ã—ã¾ã™ã€‚
"""
        return prompt
    
    def parse_claude_response(self, claude_response: str) -> Optional[Dict]:
        """
        Claude Codeã®å›ç­”ã‚’ãƒ‘ãƒ¼ã‚¹
        """
        try:
            # JSONéƒ¨åˆ†ã‚’æŠ½å‡º
            json_pattern = r'```json\s*(.*?)\s*```'
            match = re.search(json_pattern, claude_response, re.DOTALL)
            
            if match:
                json_str = match.group(1)
                sentiment_data = json.loads(json_str)
                
                # å¿…è¦ãªãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®æ¤œè¨¼
                required_fields = ['sentiment_score', 'usd_impact', 'jpy_impact', 
                                 'timeframe', 'confidence']
                
                for field in required_fields:
                    if field not in sentiment_data:
                        print(f"å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒä¸è¶³: {field}")
                        return None
                
                # æ•°å€¤ã®ç¯„å›²ãƒã‚§ãƒƒã‚¯
                for score_field in ['sentiment_score', 'usd_impact', 'jpy_impact']:
                    value = sentiment_data[score_field]
                    if not (-1.0 <= value <= 1.0):
                        print(f"ã‚¹ã‚³ã‚¢ç¯„å›²ã‚¨ãƒ©ãƒ¼ {score_field}: {value}")
                        return None
                
                if not (0.0 <= sentiment_data['confidence'] <= 1.0):
                    print(f"ä¿¡é ¼åº¦ç¯„å›²ã‚¨ãƒ©ãƒ¼: {sentiment_data['confidence']}")
                    return None
                
                # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—è¿½åŠ 
                sentiment_data['timestamp'] = datetime.now().isoformat()
                
                return sentiment_data
            
            else:
                print("JSONå½¢å¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return None
                
        except json.JSONDecodeError as e:
            print(f"JSONè§£æã‚¨ãƒ©ãƒ¼: {e}")
            return None
        except Exception as e:
            print(f"ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def add_sentiment_analysis(self, news_text: str, analysis_result: Dict) -> bool:
        """
        æ„Ÿæƒ…åˆ†æçµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«è¿½åŠ 
        """
        try:
            news_hash = str(hash(news_text.strip()))
            analysis_result['news_text'] = news_text[:200]  # æœ€åˆã®200æ–‡å­—ã®ã¿ä¿å­˜
            analysis_result['added_timestamp'] = datetime.now().isoformat()
            
            self.sentiment_cache[news_hash] = analysis_result
            self._save_sentiment_cache()
            
            print(f"æ„Ÿæƒ…åˆ†æçµæœã‚’ä¿å­˜: {analysis_result['sentiment_score']:.2f}")
            return True
            
        except Exception as e:
            print(f"æ„Ÿæƒ…åˆ†æä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def get_recent_sentiment_features(self, hours_back: int = 24) -> Dict[str, float]:
        """
        ç›´è¿‘ã®æ„Ÿæƒ…åˆ†æç‰¹å¾´é‡ã‚’å–å¾—
        """
        current_time = datetime.now()
        cutoff_time = current_time - timedelta(hours=hours_back)
        
        recent_sentiments = []
        
        # ç›´è¿‘ã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        for news_hash, analysis in self.sentiment_cache.items():
            try:
                timestamp = datetime.fromisoformat(analysis['timestamp'])
                if timestamp >= cutoff_time:
                    recent_sentiments.append(analysis)
            except (KeyError, ValueError):
                continue
        
        if not recent_sentiments:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ï¼ˆãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ©ãƒ«ï¼‰
            return {
                'news_sentiment': 0.0,
                'usd_strength': 0.0,
                'jpy_strength': 0.0,
                'market_fear': 0.0,
                'sentiment_confidence': 0.5,
                'sentiment_count': 0
            }
        
        # é‡ã¿ä»˜ãå¹³å‡è¨ˆç®—ï¼ˆæ–°ã—ã„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã»ã©é‡è¦ï¼‰
        total_weight = 0
        weighted_sentiment = 0
        weighted_usd = 0
        weighted_jpy = 0
        weighted_confidence = 0
        
        for analysis in recent_sentiments:
            # æ™‚é–“ã«ã‚ˆã‚‹é‡ã¿ï¼ˆæ–°ã—ã„ã»ã©é‡ã„ï¼‰
            timestamp = datetime.fromisoformat(analysis['timestamp'])
            hours_ago = (current_time - timestamp).total_seconds() / 3600
            time_weight = max(0.1, 1.0 - (hours_ago / hours_back))
            
            # ä¿¡é ¼åº¦ã«ã‚ˆã‚‹é‡ã¿
            confidence_weight = analysis.get('confidence', 0.5)
            
            # ç·åˆé‡ã¿
            weight = time_weight * confidence_weight
            
            weighted_sentiment += analysis['sentiment_score'] * weight
            weighted_usd += analysis['usd_impact'] * weight
            weighted_jpy += analysis['jpy_impact'] * weight
            weighted_confidence += analysis['confidence'] * weight
            total_weight += weight
        
        if total_weight == 0:
            return {
                'news_sentiment': 0.0,
                'usd_strength': 0.0,
                'jpy_strength': 0.0,
                'market_fear': 0.0,
                'sentiment_confidence': 0.5,
                'sentiment_count': 0
            }
        
        # æ­£è¦åŒ–
        features = {
            'news_sentiment': weighted_sentiment / total_weight,
            'usd_strength': weighted_usd / total_weight,
            'jpy_strength': weighted_jpy / total_weight,
            'market_fear': -weighted_sentiment / total_weight,  # ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆã®é€†
            'sentiment_confidence': weighted_confidence / total_weight,
            'sentiment_count': len(recent_sentiments)
        }
        
        return features
    
    def print_analysis_prompt(self, news_text: str):
        """
        Claude Codeç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¡¨ç¤º
        """
        importance = self.analyze_news_importance(news_text)
        
        print("=" * 80)
        print("ğŸ§  Claude Code æ„Ÿæƒ…åˆ†æãƒªã‚¯ã‚¨ã‚¹ãƒˆ")
        print("=" * 80)
        print(f"é‡è¦åº¦: {importance:.2f}/1.0")
        print()
        print(self.generate_claude_analysis_prompt(news_text))
        print("=" * 80)
        print("ğŸ‘† ä¸Šè¨˜ã‚’Claude Codeã§å®Ÿè¡Œã—ã¦ã€çµæœã‚’è²¼ã‚Šä»˜ã‘ã¦ãã ã•ã„")
        print("=" * 80)
    
    def interactive_sentiment_input(self, news_text: str) -> Optional[Dict]:
        """
        å¯¾è©±å¼æ„Ÿæƒ…åˆ†æå…¥åŠ›
        """
        print(f"\nãƒ‹ãƒ¥ãƒ¼ã‚¹: {news_text}")
        print("æ„Ÿæƒ…åˆ†æçµæœã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆEnter ã§ 0.0ï¼‰:")
        
        try:
            sentiment_score = float(input("ç·åˆæ„Ÿæƒ…ã‚¹ã‚³ã‚¢ (-1.0ã€œ1.0): ") or "0.0")
            usd_impact = float(input("USDå½±éŸ¿åº¦ (-1.0ã€œ1.0): ") or "0.0")
            jpy_impact = float(input("JPYå½±éŸ¿åº¦ (-1.0ã€œ1.0): ") or "0.0")
            timeframe = input("æ™‚é–“è»¸ (short/medium/long): ") or "short"
            confidence = float(input("ä¿¡é ¼åº¦ (0.0ã€œ1.0): ") or "0.5")
            
            analysis_result = {
                'sentiment_score': max(-1.0, min(1.0, sentiment_score)),
                'usd_impact': max(-1.0, min(1.0, usd_impact)),
                'jpy_impact': max(-1.0, min(1.0, jpy_impact)),
                'timeframe': timeframe,
                'confidence': max(0.0, min(1.0, confidence)),
                'key_factors': ['æ‰‹å‹•å…¥åŠ›'],
                'timestamp': datetime.now().isoformat()
            }
            
            return analysis_result
            
        except ValueError as e:
            print(f"å…¥åŠ›ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def get_sentiment_summary(self) -> Dict:
        """
        æ„Ÿæƒ…åˆ†æã®è¦ç´„çµ±è¨ˆ
        """
        if not self.sentiment_cache:
            return {"total_analyses": 0}
        
        total_count = len(self.sentiment_cache)
        recent_features = self.get_recent_sentiment_features()
        
        # ã‚¹ã‚³ã‚¢åˆ†å¸ƒ
        all_scores = [analysis.get('sentiment_score', 0) 
                     for analysis in self.sentiment_cache.values()]
        
        summary = {
            'total_analyses': total_count,
            'recent_sentiment': recent_features['news_sentiment'],
            'recent_usd_strength': recent_features['usd_strength'],
            'recent_confidence': recent_features['sentiment_confidence'],
            'avg_sentiment': np.mean(all_scores) if all_scores else 0.0,
            'sentiment_volatility': np.std(all_scores) if all_scores else 0.0
        }
        
        return summary


def demo_sentiment_analyzer():
    """
    æ„Ÿæƒ…åˆ†æã‚·ã‚¹ãƒ†ãƒ ã®ãƒ‡ãƒ¢
    """
    analyzer = ClaudeSentimentAnalyzer()
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‹ãƒ¥ãƒ¼ã‚¹
    sample_news = [
        "FRBãŒ0.75%ã®å¤§å¹…åˆ©ä¸Šã’ã‚’æ±ºå®šã€ã‚¤ãƒ³ãƒ•ãƒ¬æŠ‘åˆ¶ã‚’æœ€å„ªå…ˆ",
        "æ—¥éŠ€ç·è£ã€é‡‘èç·©å’Œæ”¿ç­–ã®ç¶™ç¶šã‚’è¡¨æ˜ã€å††å®‰é€²è¡Œã®æ‡¸å¿µ",
        "ç±³é›‡ç”¨çµ±è¨ˆãŒäºˆæƒ³ã‚’å¤§å¹…ä¸Šå›ã‚‹ã€åŠ´åƒå¸‚å ´ã®å …èª¿ã•ç¤ºã™"
    ]
    
    print("ğŸ§  Claudeæ„Ÿæƒ…åˆ†æã‚·ã‚¹ãƒ†ãƒ  ãƒ‡ãƒ¢")
    print("=" * 60)
    
    for news in sample_news:
        importance = analyzer.analyze_news_importance(news)
        print(f"\nãƒ‹ãƒ¥ãƒ¼ã‚¹: {news}")
        print(f"é‡è¦åº¦: {importance:.2f}")
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
        prompt = analyzer.generate_claude_analysis_prompt(news)
        print("ç”Ÿæˆã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ:")
        print(prompt[:200] + "...")
    
    # æ„Ÿæƒ…ç‰¹å¾´é‡å–å¾—
    features = analyzer.get_recent_sentiment_features()
    print(f"\nç¾åœ¨ã®æ„Ÿæƒ…ç‰¹å¾´é‡:")
    for key, value in features.items():
        print(f"  {key}: {value:.3f}")


if __name__ == "__main__":
    demo_sentiment_analyzer()